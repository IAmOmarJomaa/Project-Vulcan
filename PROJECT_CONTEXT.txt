LOCATION: harvest_context.py
----------------------------------------
import os

# Define the root of your project
ROOT_DIR = os.getcwd()
OUTPUT_FILE = "PROJECT_CONTEXT.txt"

# Folders to ignore to keep the context clean
IGNORE_DIRS = {'.git', '__pycache__', 'data', 'venv', 'node_modules'}
IGNORE_FILES = {OUTPUT_FILE, '.env', 'package-lock.json'}

def harvest_context():
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as outfile:
        for root, dirs, files in os.walk(ROOT_DIR):
            # Prune ignored directories
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]
            
            for file in files:
                if file in IGNORE_FILES or file.endswith(('.pyc', '.png', '.jpg', '.parquet')):
                    continue
                
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, ROOT_DIR)
                
                outfile.write(f"LOCATION: {relative_path}\n")
                outfile.write("-" * 40 + "\n")
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        outfile.write(f.read())
                except Exception as e:
                    outfile.write(f"[ERROR READING FILE: {e}]")
                
                outfile.write("\n\n" + "="*80 + "\n\n")

    print(f"‚úÖ Success! All code gathered in {OUTPUT_FILE}")

if __name__ == "__main__":
    harvest_context()

================================================================================

LOCATION: .env.example
----------------------------------------
COMFYUI_SERVER=192.168.112.1:8188
OUTPUT_DIR=final_showcase


================================================================================

LOCATION: .gitignore
----------------------------------------
# AI Models & Weights
*.safetensors
*.gguf
*.sft
*.ckpt

# Output Assets
final_showcase/
*.png
*.jpg
*.jpeg

# Environment & System
.env
venv/
__pycache__/
.DS_Store


================================================================================

LOCATION: requirements.txt
----------------------------------------
websocket-client
python-dotenv
matplotlib
Pillow


================================================================================

LOCATION: tools/harvest_context.py
----------------------------------------
import os

# Define the root of your project
ROOT_DIR = os.getcwd()
OUTPUT_FILE = "PROJECT_CONTEXT.txt"

# Folders to ignore to keep the context clean
IGNORE_DIRS = {'.git', '__pycache__', 'data', 'venv', 'node_modules'}
IGNORE_FILES = {OUTPUT_FILE, '.env', 'package-lock.json'}

def harvest_context():
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as outfile:
        for root, dirs, files in os.walk(ROOT_DIR):
            # Prune ignored directories
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]
            
            for file in files:
                if file in IGNORE_FILES or file.endswith(('.pyc', '.png', '.jpg', '.parquet')):
                    continue
                
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, ROOT_DIR)
                
                outfile.write(f"LOCATION: {relative_path}\n")
                outfile.write("-" * 40 + "\n")
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        outfile.write(f.read())
                except Exception as e:
                    outfile.write(f"[ERROR READING FILE: {e}]")
                
                outfile.write("\n\n" + "="*80 + "\n\n")

    print(f"‚úÖ Success! All code gathered in {OUTPUT_FILE}")

if __name__ == "__main__":
    harvest_context()

================================================================================

LOCATION: tools/create_stylebook.py
----------------------------------------
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os
import glob
import textwrap

# --- CONFIGURATION ---
IMAGE_FOLDER = "final_showcase"
OUTPUT_FILE = "Final_Presentation_Board.jpg"
ROWS = 3
COLS = 5
FIG_SIZE = (20, 15) # High resolution size (width, height)

# --- THE DATA (For captions) ---
SCENARIOS = [
    {"name": "Corporate CEO", "prompt": "Caitlyn as a tech CEO giving a keynote speech, modern auditorium, confident smile."},
    {"name": "Coffee Ad", "prompt": "Caitlyn holding a steaming ceramic coffee cup, cozy sweater, rainy window cafe."},
    {"name": "Luxury Perfume", "prompt": "Caitlyn in an elegant evening gown, holding a crystal perfume bottle, studio lighting."},
    {"name": "Fitness Brand", "prompt": "Caitlyn jogging in a modern city park at sunrise, premium athletic wear, dynamic pose."},
    {"name": "Doctor", "prompt": "Caitlyn dressed as a professional doctor with a stethoscope, white coat, hospital background."},
    {"name": "Travel Paris", "prompt": "Caitlyn taking a selfie with the Eiffel Tower, golden hour, chic beret and trench coat."},
    {"name": "Cozy Reading", "prompt": "Caitlyn reading a book in a comfortable armchair, surrounded by plants, library setting."},
    {"name": "Summer Beach", "prompt": "Caitlyn walking on a white sand beach, wearing a white linen dress, sunny day."},
    {"name": "Urban Style", "prompt": "Caitlyn leaning against a brick wall in New York, leather jacket, sunglasses, neon."},
    {"name": "Cooking Chef", "prompt": "Caitlyn in a modern kitchen wearing a chef's apron, preparing a gourmet salad."},
    {"name": "SciFi Cyberpunk", "prompt": "Caitlyn in a futuristic cyberpunk city, neon rain, high-tech tactical jacket."},
    {"name": "Fantasy Elf", "prompt": "Caitlyn as an ethereal elf queen in a magical forest, silver tiara, glowing fireflies."},
    {"name": "Space Explorer", "prompt": "Caitlyn wearing a white space suit inside a spaceship corridor, looking at Earth."},
    {"name": "Victorian Era", "prompt": "Portrait of Caitlyn in 1890s Victorian era clothing, high collar lace dress, sepia."},
    {"name": "Abstract Art", "prompt": "Double exposure artistic portrait of Caitlyn combined with a forest landscape."}
]

def create_grid():
    # 1. Get Images
    # Sort ensures 01 matches Scenario 1, 02 matches Scenario 2, etc.
    image_files = sorted(glob.glob(os.path.join(IMAGE_FOLDER, "*.png")))
    
    if not image_files:
        print("‚ùå No images found in final_showcase!")
        return

    print(f"üé® Creating {ROWS}x{COLS} grid from {len(image_files)} images...")

    # 2. Setup Plot
    # constrained_layout helps prevent text overlap automatically
    fig, axes = plt.subplots(ROWS, COLS, figsize=FIG_SIZE, constrained_layout=True)
    
    # Flatten axes array for easy looping
    axes_flat = axes.flatten()

    # 3. Fill the Grid
    for i, ax in enumerate(axes_flat):
        if i < len(image_files) and i < len(SCENARIOS):
            img_path = image_files[i]
            scenario = SCENARIOS[i]
            
            try:
                # Load and Display Image
                img = mpimg.imread(img_path)
                ax.imshow(img)
                
                # --- CAPTION STYLING ---
                # Title (Bold)
                title = scenario['name']
                # Prompt (Wrapped, smaller)
                wrapped_prompt = "\n".join(textwrap.wrap(scenario['prompt'], width=30))
                
                label_text = f"{title}\n\n{wrapped_prompt}"
                
                # Set text below image (xlabel)
                ax.set_xlabel(label_text, fontsize=9, labelpad=10, linespacing=1.4)
                
                # Clean up borders
                ax.set_xticks([]) # Remove X ticks
                ax.set_yticks([]) # Remove Y ticks
                
                # Remove the black box outline (spines)
                for spine in ax.spines.values():
                    spine.set_visible(False)
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Error reading {img_path}: {e}")
        else:
            # Hide unused squares
            ax.axis('off')

    # 4. Add Main Title
    fig.suptitle('PROJECT VULCAN: Digital Twin Scenarios', fontsize=20, weight='bold')

    # 5. Save
    print("üíæ Saving high-res image...")
    # facecolor='white' ensures the background is white, not transparent
    plt.savefig(OUTPUT_FILE, dpi=200, facecolor='white')
    print(f"‚úÖ Done! Saved to: {OUTPUT_FILE}")

if __name__ == "__main__":
    create_grid()

================================================================================

LOCATION: tools/rapport_img.py
----------------------------------------
import os
import glob
from PIL import Image, ImageOps

# --- CONFIGURATION ---
# Chemin vers ton dossier d'images (Format WSL)
IMAGE_FOLDER = "/home/iamomarjomaa/gen_ai/selections"
OUTPUT_FILE = "dataset_album_pro.jpg"

# Combien d'images veux-tu par ligne ? (3 ou 4 est conseill√©)
IMAGES_PER_ROW = 3

# Largeur cible de l'image finale en pixels (3000 = haute r√©solution pour impression)
TARGET_WIDTH = 3000

# Espace entre les images (padding) en pixels
PADDING = 20
# ---------------------


def create_smart_collage(image_paths, output_path, imgs_per_row, target_width, padding):
    """Cr√©e un collage intelligent en respectant les ratios."""
    
    # 1. Charger toutes les images en m√©moire
    loaded_images = []
    print(f"üìÇ Chargement de {len(image_paths)} images...")
    for p in image_paths:
        try:
            img = Image.open(p)
            # Convertir en RGB si n√©cessaire (pour √©viter probl√®mes avec PNG transparents)
            if img.mode != 'RGB':
                img = img.convert('RGB')
            loaded_images.append(img)
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible de lire {p}: {e}")

    if not loaded_images:
        print("‚ùå Aucune image valide trouv√©e.")
        exit()

    # Limiter au nombre d'images souhait√© pour le rapport (ex: les 10 premi√®res)
    # loaded_images = loaded_images[:12] 

    # 2. Diviser en lignes (chunks)
    rows = [loaded_images[i:i + imgs_per_row] for i in range(0, len(loaded_images), imgs_per_row)]
    
    processed_rows = []
    total_height = 0

    print(f"üìê Calcul de la mise en page pour {len(rows)} lignes...")

    # 3. Traiter chaque ligne
    for row_imgs in rows:
        # Calculer la largeur disponible pour les images (largeur totale - les espaces)
        available_width = target_width - (padding * (len(row_imgs) + 1))
        
        # Somme des ratios d'aspect (largeur / hauteur) de la ligne
        # Cela permet de trouver la hauteur commune id√©ale
        aspect_ratio_sum = sum([img.width / img.height for img in row_imgs])
        
        # Hauteur id√©ale pour que cette ligne remplisse exactement la largeur disponible
        row_height = int(available_width / aspect_ratio_sum)
        
        resized_imgs_in_row = []
        current_row_width = 0
        
        # Redimensionner chaque image de la ligne √† cette nouvelle hauteur
        for img in row_imgs:
            new_width = int(img.width * (row_height / img.height))
            resized_img = img.resize((new_width, row_height), Image.Resampling.LANCZOS)
            resized_imgs_in_row.append(resized_img)
            current_row_width += new_width

        processed_rows.append({
            'images': resized_imgs_in_row,
            'height': row_height,
            'width': current_row_width # Largeur r√©elle occup√©e par les images
        })
        total_height += row_height + padding

    # Ajouter le padding final en bas
    total_height += padding

    # 4. Cr√©er la toile blanche finale
    print(f"üé® Cr√©ation de l'image finale ({target_width}x{total_height} px)...")
    collage = Image.new('RGB', (target_width, total_height), 'white')
    
    current_y = padding

    # 5. Coller les images
    for row_data in processed_rows:
        # Centrer la ligne horizontalement si elle est un peu moins large que le target
        row_content_width = row_data['width'] + (padding * (len(row_data['images']) - 1))
        start_x = (target_width - row_content_width) // 2
        
        current_x = start_x
        
        for img in row_data['images']:
            collage.paste(img, (current_x, current_y))
            current_x += img.width + padding
            
        current_y += row_data['height'] + padding

    # 6. Sauvegarder
    collage.save(output_path, quality=95)
    print(f"‚úÖ Succ√®s ! Album sauvegard√© sous : {output_path}")


# --- Ex√©cution ---
if __name__ == "__main__":
    # V√©rifier le chemin
    if not os.path.exists(IMAGE_FOLDER):
        print(f"‚ùå Erreur : Le dossier {IMAGE_FOLDER} n'existe pas.")
        exit()

    # Trouver les images
    extensions = ["*.png", "*.jpg", "*.jpeg", "*.PNG", "*.JPG"]
    image_files = []
    for ext in extensions:
        image_files.extend(glob.glob(os.path.join(IMAGE_FOLDER, ext)))
    
    # Trier pour avoir un ordre coh√©rent
    image_files.sort()

    if not image_files:
        print("‚ùå Erreur : Aucune image trouv√©e dans le dossier.")
        exit()
        
    # Lancer la cr√©ation
    create_smart_collage(image_files, OUTPUT_FILE, IMAGES_PER_ROW, TARGET_WIDTH, PADDING)

================================================================================

LOCATION: core/director.py
----------------------------------------
import websocket
import uuid
import json
import urllib.request
import urllib.parse
import random
import os
from dotenv import load_dotenv

load_dotenv()

# --- CONFIGURATION ---
SERVER_ADDRESS = os.getenv("COMFYUI_SERVER", "192.168.112.1:8188") # 
OUTPUT_FOLDER = os.getenv("OUTPUT_DIR", "final_showcase") # [cite: 107, 205]
CLIENT_ID = str(uuid.uuid4())

class VulcanDirector:
    """
    Orchestration Layer for Hybrid Cloud-Edge Generative Workflows.
    Manages WebSocket lifecycle and dynamic prompt injection.
    """
    def __init__(self, server_address: str, client_id: str):
        self.server_address = server_address
        self.client_id = client_id
        self.ws = websocket.WebSocket()

    def connect(self):
        """Establishes the WebSocket bridge between WSL2 and Windows."""
        try:
            self.ws.connect(f"ws://{self.server_address}/ws?clientId={self.client_id}") # 
            print(f"üì° Connected to Engine: {self.server_address}")
        except Exception as e:
            print(f"‚ùå Connection Failed: {e}")
            raise

    def queue_prompt(self, prompt_workflow: dict):
        """Dispatches the JSON DAG to the ComfyUI API."""
        p = {"prompt": prompt_workflow, "client_id": self.client_id}
        data = json.dumps(p).encode('utf-8')
        req = urllib.request.Request(f"http://{self.server_address}/prompt", data=data)
        return json.loads(urllib.request.urlopen(req).read())

    def get_images(self, workflow: dict):
        """
        Monitors execution state and retrieves binary image data.
        Implementation of the Observer Pattern.
        """
        prompt_id = self.queue_prompt(workflow)['prompt_id']
        while True:
            out = self.ws.recv()
            if isinstance(out, str):
                message = json.loads(out)
                if message['type'] == 'executing':
                    data = message['data']
                    if data['node'] is None and data['prompt_id'] == prompt_id:
                        break # [cite: 187, 201]
        
        # History Retrieval Logic
        with urllib.request.urlopen(f"http://{self.server_address}/history/{prompt_id}") as response:
            history = json.loads(response.read())[prompt_id]
            
        return history['outputs']

    def save_output(self, node_outputs: dict, scene_name: str, index: int):
        """Saves generated tensors as local PNG assets."""
        if not os.path.exists(OUTPUT_FOLDER):
            os.makedirs(OUTPUT_FOLDER) # [cite: 205]

        for node_id, output in node_outputs.items():
            if 'images' in output:
                for img in output['images']:
                    params = urllib.parse.urlencode({
                        "filename": img['filename'], 
                        "subfolder": img['subfolder'], 
                        "type": img['type']
                    })
                    with urllib.request.urlopen(f"http://{self.server_address}/view?{params}") as resp:
                        filename = f"{OUTPUT_FOLDER}/{str(index).zfill(2)}_{scene_name}.png"
                        with open(filename, "wb") as f:
                            f.write(resp.read()) # [cite: 187, 205]
                        print(f"‚úÖ Asset Saved: {filename}")

# --- EXECUTION LOGIC ---
if __name__ == "__main__":
    director = VulcanDirector(SERVER_ADDRESS, CLIENT_ID)
    director.connect()
    
    # Load your optimized GGUF workflow [cite: 174, 178, 183]
    with open("workflows/flux_api_workflow.json", "r", encoding="utf-8") as f:
        active_workflow = json.load(f)

    # Example: Run Scenario 1 (Corporate CEO) [cite: 219]
    # In a full run, you would loop through your SCENARIOS list here.

================================================================================

LOCATION: core/instant_director.py
----------------------------------------
import websocket # pip install websocket-client
import uuid
import json
import urllib.request
import urllib.parse
import random
import os
import sys

# --- CONFIGURATION ---
# Your Windows IP (Check if it changed!)
SERVER_ADDRESS = "192.168.112.1:8188" 
CLIENT_ID = str(uuid.uuid4())

def queue_prompt(prompt):
    p = {"prompt": prompt, "client_id": CLIENT_ID}
    data = json.dumps(p).encode('utf-8')
    req = urllib.request.Request(f"http://{SERVER_ADDRESS}/prompt", data=data)
    return json.loads(urllib.request.urlopen(req).read())

def get_image(filename, subfolder, folder_type):
    data = {"filename": filename, "subfolder": subfolder, "type": folder_type}
    url_values = urllib.parse.urlencode(data)
    with urllib.request.urlopen(f"http://{SERVER_ADDRESS}/view?{url_values}") as response:
        return response.read()

def get_history(prompt_id):
    with urllib.request.urlopen(f"http://{SERVER_ADDRESS}/history/{prompt_id}") as response:
        return json.loads(response.read())

def get_images(ws, workflow):
    prompt_id = queue_prompt(workflow)['prompt_id']
    output_images = {}
    while True:
        out = ws.recv()
        if isinstance(out, str):
            message = json.loads(out)
            if message['type'] == 'executing':
                data = message['data']
                if data['node'] is None and data['prompt_id'] == prompt_id:
                    break # Execution is done
        else:
            continue

    history = get_history(prompt_id)[prompt_id]
    for node_id in history['outputs']:
        node_output = history['outputs'][node_id]
        if 'images' in node_output:
            images_output = []
            for image in node_output['images']:
                image_data = get_image(image['filename'], image['subfolder'], image['type'])
                images_output.append(image_data)
            output_images[node_id] = images_output

    return output_images

# --- MAIN LOOP ---
if __name__ == "__main__":
    # 1. Load Workflow
    if not os.path.exists("flux_api_workflow.json"):
        print("‚ùå Error: 'flux_api_workflow.json' missing.")
        sys.exit()

    with open("flux_api_workflow.json", "r", encoding="utf-8") as f:
        workflow = json.load(f)

    # 2. Connect
    print(f"üì° Connected to Engine at {SERVER_ADDRESS}")
    ws = websocket.WebSocket()
    try:
        ws.connect(f"ws://{SERVER_ADDRESS}/ws?clientId={CLIENT_ID}")
    except Exception as e:
        print(f"‚ùå Connection Error: {e}")
        sys.exit()

    print("üöÄ Instant Director Ready! (Type 'exit' to quit)")
    print("------------------------------------------------")

    # 3. Interactive Loop
    while True:
        try:
            # Get input from user
            user_input = input("\nüé® Enter Prompt: ")
            
            if user_input.lower() in ["exit", "quit", "q"]:
                print("üëã Goodbye!")
                break
            
            if not user_input.strip():
                continue

            print("‚è≥ Generating...")

            # Inject Prompt & Random Seed
            workflow["6"]["inputs"]["text"] = user_input
            workflow["3"]["inputs"]["seed"] = random.randint(1, 10**10)

            # Execute
            images = get_images(ws, workflow)

            # Save & Open
            for node_id in images:
                for image_data in images[node_id]:
                    # Create a clean filename from the first few words of the prompt
                    safe_name = "".join([c for c in user_input[:20] if c.isalnum() or c==' ']).strip().replace(" ", "_")
                    filename = f"instant_{safe_name}.png"
                    
                    with open(filename, "wb") as f:
                        f.write(image_data)
                    
                    print(f"‚úÖ Saved: {filename}")
                    
                    # MAGIC COMMAND: Open image in Windows
                    print("üëÄ Opening...")
                    os.system(f"explorer.exe {filename}")

        except KeyboardInterrupt:
            print("\nüëã Exiting...")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è Error: {e}")

================================================================================

LOCATION: workflows/flux_api_workflow.json
----------------------------------------
{
  "3": {
    "inputs": {
      "seed": 102030405060,
      "steps": 20,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 1,
      "model": [
        "10",
        0
      ],
      "positive": [
        "6",
        0
      ],
      "negative": [
        "7",
        0
      ],
      "latent_image": [
        "5",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "4": {
    "inputs": {
      "unet_name": "flux1-dev-Q4_K_S.gguf"
    },
    "class_type": "UnetLoaderGGUF",
    "_meta": {
      "title": "Unet Loader (GGUF)"
    }
  },
  "5": {
    "inputs": {
      "width": 832,
      "height": 1216,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "6": {
    "inputs": {
      "text": "Caitlyn in a beige business suit, holding a coffee cup, cinematic lighting, highly detailed, 4k",
      "clip": [
        "11",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Positive)"
    }
  },
  "7": {
    "inputs": {
      "text": "blur, distortion, low quality",
      "clip": [
        "11",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Negative)"
    }
  },
  "8": {
    "inputs": {
      "samples": [
        "3",
        0
      ],
      "vae": [
        "9",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "9": {
    "inputs": {
      "vae_name": "ae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "10": {
    "inputs": {
      "lora_name": "caitlyn_lifestyle_v1.safetensors",
      "strength_model": 1,
      "strength_clip": 1,
      "model": [
        "4",
        0
      ],
      "clip": [
        "11",
        0
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "11": {
    "inputs": {
      "clip_name1": "clip_l.safetensors",
      "clip_name2": "t5xxl_fp8_e4m3fn.safetensors",
      "type": "flux",
      "device": "default"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "Dual CLIP Loader"
    }
  },
  "12": {
    "inputs": {
      "filename_prefix": "Vulcan_Output",
      "images": [
        "8",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  }
}

================================================================================

LOCATION: workflows/flux_gguf_workflow.json
----------------------------------------
{
  "3": {
    "inputs": {
      "seed": 102030405060,
      "steps": 20,
      "cfg": 1.0,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 1.0,
      "model": ["10", 0],
      "positive": ["6", 0],
      "negative": ["7", 0],
      "latent_image": ["5", 0]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "4": {
    "inputs": {
      "ckpt_name": "flux1-dev-Q4_K_S.gguf"
    },
    "class_type": "UnetLoaderGGUF",
    "_meta": {
      "title": "Unet Loader (GGUF)"
    }
  },
  "5": {
    "inputs": {
      "width": 832,
      "height": 1216,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "6": {
    "inputs": {
      "text": "Caitlyn in a beige business suit, holding a coffee cup, cinematic lighting, highly detailed, 4k",
      "clip": ["11", 0]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Positive)"
    }
  },
  "7": {
    "inputs": {
      "text": "blur, distortion, low quality",
      "clip": ["11", 0]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Negative)"
    }
  },
  "8": {
    "inputs": {
      "samples": ["3", 0],
      "vae": ["9", 0]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "9": {
    "inputs": {
      "vae_name": "ae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "10": {
    "inputs": {
      "lora_name": "caitlyn_lifestyle_v1.safetensors",
      "strength_model": 1.0,
      "strength_clip": 1.0,
      "model": ["4", 0],
      "clip": ["11", 0]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "11": {
    "inputs": {
      "clip_name1": "clip_l.safetensors",
      "clip_name2": "t5xxl_fp8_e4m3fn.safetensors",
      "type": "flux"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "Dual CLIP Loader"
    }
  },
  "12": {
    "inputs": {
      "filename_prefix": "Vulcan_Output",
      "images": ["8", 0]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  }
}

================================================================================

